# -*- coding: utf-8 -*-
"""Bill_training_take1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LMV5UrKrLREctu_uaZT2RnGqsRhbGA_E
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def process_single_csv(filepath, region_name):
    """
    Process a single CSV file with all data engineering steps
    """
    print(f"\n{'='*50}")
    print(f"Processing {region_name} data from {filepath}")
    print('='*50)

    # Read the CSV file
    df = pd.read_csv(filepath)
    print(f"Original shape: {df.shape}")
    print(f"Columns: {df.columns.tolist()}")

    # ===== STEP 1: Convert Timestamp and filter data after 7/23 15:20 =====
    print("\n1. Converting Timestamp and filtering data...")

    # Convert to datetime first
    df['Timestamp'] = pd.to_datetime(df['Timestamp'])

    # Check for null timestamps before filtering
    null_timestamps = df['Timestamp'].isnull().sum()
    if null_timestamps > 0:
        print(f"   WARNING: {null_timestamps} null timestamps found. Removing these rows.")
        df = df.dropna(subset=['Timestamp'])

    # Create cutoff datetime (assuming year 2025 based on your data)
    cutoff_date = pd.to_datetime('2025-07-23 15:20:00+08:00')
    print(f"   Cutoff date: {cutoff_date}")

    # Filter data
    original_len = len(df)
    df = df[df['Timestamp'] > cutoff_date].copy()
    filtered_len = len(df)
    print(f"   Filtered out {original_len - filtered_len} rows before cutoff")
    print(f"   Remaining rows: {filtered_len}")

    # ===== STEP 2: Shift weather data up one row (except for 'Other' region) =====
    weather_columns = ['AirTemperature', 'WindSpeed', 'SunshineDuration', 'Precipitation']

    if region_name != 'Other':
        print("\n2. Shifting weather data up one row...")

        # Check which weather columns exist
        existing_weather_cols = [col for col in weather_columns if col in df.columns]

        if existing_weather_cols:
            # Store the original values for logging
            original_first_weather = df[existing_weather_cols].iloc[0].to_dict()

            # Shift weather data up by 1 row
            for col in existing_weather_cols:
                df[col] = df[col].shift(-1)

            print(f"   Shifted columns: {existing_weather_cols}")
            print(f"   Example: First row's weather data now contains what was originally in second row")

            # Remove the last row which now has NaN weather values
            print("   Removing last row (has no weather data after shift)...")
            df = df.iloc[:-1].copy()
            print(f"   New shape after removing last row: {df.shape}")
    else:
        print("\n2. Skipping weather shift for 'Other' region...")

    # ===== STEP 3: Extract datetime features =====
    print("\n3. Extracting datetime features from Timestamp...")

    # Extract datetime components
    df['Year'] = df['Timestamp'].dt.year
    df['Month'] = df['Timestamp'].dt.month
    df['Day'] = df['Timestamp'].dt.day
    df['DayOfWeek'] = df['Timestamp'].dt.dayofweek  # Monday=0, Sunday=6
    df['Hour'] = df['Timestamp'].dt.hour
    df['Minute'] = df['Timestamp'].dt.minute

    print("   Datetime features extracted: Year, Month, Day, DayOfWeek, Hour, Minute")

    # ===== STEP 4: Remove Timestamp column =====
    print("\n4. Removing Timestamp column...")
    df = df.drop('Timestamp', axis=1)
    print(f"   New shape after removing Timestamp: {df.shape}")

    # ===== STEP 5: Handle weather columns for 'Other' region =====
    if region_name == 'Other':
        print("\n5. Special handling for 'Other' region - removing weather columns...")
        existing_weather_cols = [col for col in weather_columns if col in df.columns]
        if existing_weather_cols:
            df = df.drop(existing_weather_cols, axis=1)
            print(f"   Removed weather columns: {existing_weather_cols}")
        print(f"   Shape after removing weather columns: {df.shape}")

    # ===== STEP 6: Check for null values =====
    print("\n6. Checking for null values in each column...")
    null_counts = df.isnull().sum()
    columns_with_nulls = null_counts[null_counts > 0]

    if len(columns_with_nulls) > 0:
        print("   Columns with null values:")
        for col, null_count in columns_with_nulls.items():
            print(f"   - {col}: {null_count} nulls ({null_count/len(df)*100:.2f}%)")
    else:
        print("   No null values found!")

    # ===== STEP 7: Fill null values with column average (commented out) =====
    print("\n7. Code to fill null values with column average (currently commented out):")
    print("   Uncomment the code block below to enable null filling")

    # === UNCOMMENT THIS BLOCK TO FILL NULL VALUES WITH COLUMN AVERAGE ===
    for column in df.columns:
         if df[column].isnull().any():
             # Calculate mean only for numeric columns
             if pd.api.types.is_numeric_dtype(df[column]):
                 column_mean = df[column].mean()
                 df[column].fillna(column_mean, inplace=True)
                 print(f"   Filled {column} nulls with mean value: {column_mean:.2f}")
    # =====================================================================

    # Alternative approach: Forward fill then backward fill (also commented out)
    # === UNCOMMENT THIS BLOCK FOR FORWARD/BACKWARD FILL ===
    # for column in df.columns:
    #     if df[column].isnull().any():
    #         df[column].fillna(method='ffill', inplace=True)
    #         df[column].fillna(method='bfill', inplace=True)
    #         print(f"   Filled {column} using forward/backward fill")
    # ======================================================

    # Reset index after all operations
    df = df.reset_index(drop=True)
    print(f"\nFinal shape: {df.shape}")

    return df

def prepare_sequences(df, sequence_length=6, forecast_steps=144):
    """
    Create sequences for LSTM training
    sequence_length: number of past time steps to use (6 = 1 hour)
    forecast_steps: number of future time steps to predict (144 = 24 hours)
    """
    # Define fuel columns (these are our targets)
    fuel_columns = ['Nuclear', 'Coal', 'Co-Gen', 'IPP-Coal', 'LNG', 'IPP-LNG',
                    'Oil', 'Diesel', 'Hydro', 'Wind', 'Solar', 'Other_Renewable', 'Storage']

    # All columns except Total_Generation (we'll predict individual fuels, not total)
    feature_columns = [col for col in df.columns if col != 'Total_Generation']

    # Separate features and targets
    features = df[feature_columns].values
    targets = df[fuel_columns].values

    X, y = [], []

    # Create sequences using sliding window
    for i in range(len(features) - sequence_length - forecast_steps + 1):
        # Input: sequence_length time steps of all features
        X.append(features[i:i+sequence_length])

        # Output: next forecast_steps of fuel generation only
        y.append(targets[i+sequence_length:i+sequence_length+forecast_steps])

    return np.array(X), np.array(y), feature_columns, fuel_columns

# =====================================
# MODEL ARCHITECTURE
# =====================================

def build_lstm_model(input_shape, n_fuel_types=13, forecast_steps=144):
    """
    Build LSTM Seq2Seq model with attention-like mechanism

    Architecture explanation:
    1. Input: (batch_size, sequence_length=6, n_features)
    2. LSTM layers extract temporal patterns
    3. Dense layers process the patterns
    4. Output: (batch_size, forecast_steps=144, n_fuel_types=13)
    """
    model = keras.Sequential([
        # === ENCODER PART ===
        # First LSTM layer: Learn temporal patterns from input sequence
        layers.LSTM(256, return_sequences=True, input_shape=input_shape),
        layers.Dropout(0.2),  # Prevent overfitting

        # Second LSTM layer: Further process temporal features
        layers.LSTM(128, return_sequences=False),  # return_sequences=False to get final hidden state
        layers.Dropout(0.2),

        # === DECODER PART ===
        # Dense layers to process LSTM output and generate predictions
        layers.Dense(512, activation='relu'),
        layers.Dropout(0.3),

        layers.Dense(256, activation='relu'),
        layers.Dropout(0.2),

        # Output layer: Generate all predictions at once
        # Output size = forecast_steps * n_fuel_types = 144 * 13 = 1,872
        layers.Dense(forecast_steps * n_fuel_types),

        # Reshape to (batch_size, forecast_steps, n_fuel_types)
        layers.Reshape((forecast_steps, n_fuel_types))
    ])

    # Compile model with optimizer and loss function
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.001),
        loss='mse',  # Mean Squared Error for regression
        metrics=['mae']  # Also track Mean Absolute Error
    )

    return model

def train_single_model(df, region_name, sequence_length=6, forecast_steps=144,
                      epochs=50, batch_size=32, validation_split=0.2):
    """
    Train a model for a single region
    """
    print(f"\n{'='*60}")
    print(f"Training model for {region_name}")
    print('='*60)

    # === STEP 1: Prepare sequences ===
    print("\n1. Creating sequences...")
    X, y, feature_cols, fuel_cols = prepare_sequences(df, sequence_length, forecast_steps)
    print(f"   X shape: {X.shape} (samples, time_steps, features)")
    print(f"   y shape: {y.shape} (samples, future_time_steps, fuel_types)")

    # === STEP 2: Scale the data ===
    print("\n2. Scaling features...")
    # We need to reshape X for scaling: (samples*time_steps, features)
    n_samples, n_steps, n_features = X.shape
    X_reshaped = X.reshape(-1, n_features)

    # Fit scaler and transform
    scaler_X = StandardScaler()
    X_scaled = scaler_X.fit_transform(X_reshaped)
    X_scaled = X_scaled.reshape(n_samples, n_steps, n_features)

    # Scale targets
    y_reshaped = y.reshape(-1, len(fuel_cols))
    scaler_y = StandardScaler()
    y_scaled = scaler_y.fit_transform(y_reshaped)
    y_scaled = y_scaled.reshape(n_samples, forecast_steps, len(fuel_cols))

    print("   Scaling completed")

    # === STEP 3: Split data (chronologically) ===
    print(f"\n3. Splitting data (keeping last {validation_split*100:.0f}% for validation)...")
    # Don't shuffle for time series - keep chronological order
    split_idx = int(len(X_scaled) * (1 - validation_split))

    X_train = X_scaled[:split_idx]
    X_val = X_scaled[split_idx:]
    y_train = y_scaled[:split_idx]
    y_val = y_scaled[split_idx:]

    print(f"   Training samples: {len(X_train)}")
    print(f"   Validation samples: {len(X_val)}")

    # === STEP 4: Build and train model ===
    print("\n4. Building LSTM model...")
    model = build_lstm_model(
        input_shape=(sequence_length, n_features),
        n_fuel_types=len(fuel_cols),
        forecast_steps=forecast_steps
    )

    # Print model summary
    print("\n   Model architecture:")
    model.summary()

    # Define callbacks for training
    callbacks = [
        # Stop training if validation loss doesn't improve
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=10,
            restore_best_weights=True,
            verbose=1
        ),
        # Reduce learning rate when stuck
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-6,
            verbose=1
        )
    ]

    print(f"\n5. Training model for {epochs} epochs...")
    print("   This may take several minutes...")

    # Train the model
    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        verbose=1  # Show training progress
    )

    # === STEP 5: Make predictions for evaluation ===
    print("\n6. Making predictions on validation set...")
    y_pred_scaled = model.predict(X_val)

    # Inverse transform predictions to original scale
    y_pred_reshaped = y_pred_scaled.reshape(-1, len(fuel_cols))
    y_pred = scaler_y.inverse_transform(y_pred_reshaped)
    y_pred = y_pred.reshape(-1, forecast_steps, len(fuel_cols))

    # Inverse transform true values
    y_val_reshaped = y_val.reshape(-1, len(fuel_cols))
    y_true = scaler_y.inverse_transform(y_val_reshaped)
    y_true = y_true.reshape(-1, forecast_steps, len(fuel_cols))

    # Store important objects for later use
    model_data = {
        'model': model,
        'scaler_X': scaler_X,
        'scaler_y': scaler_y,
        'history': history,
        'feature_columns': feature_cols,
        'fuel_columns': fuel_cols,
        'y_true': y_true,
        'y_pred': y_pred
    }

    return model_data

# =====================================
# ERROR CALCULATION FUNCTIONS
# =====================================

def calculate_errors(y_true, y_pred, fuel_columns, region_name):
    """
    Calculate comprehensive error metrics
    """
    print(f"\n{'='*60}")
    print(f"Error Metrics for {region_name}")
    print('='*60)

    # === OVERALL METRICS (across all samples, time steps, and fuels) ===
    print("\n1. OVERALL METRICS (all predictions combined):")

    # Flatten all dimensions for overall metrics
    y_true_flat = y_true.flatten()
    y_pred_flat = y_pred.flatten()

    overall_mse = mean_squared_error(y_true_flat, y_pred_flat)
    overall_rmse = np.sqrt(overall_mse)
    overall_mae = mean_absolute_error(y_true_flat, y_pred_flat)
    overall_r2 = r2_score(y_true_flat, y_pred_flat)

    print(f"   MSE:  {overall_mse:,.2f} MW²")
    print(f"   RMSE: {overall_rmse:,.2f} MW")
    print(f"   MAE:  {overall_mae:,.2f} MW")
    print(f"   R²:   {overall_r2:.4f}")

    # === PER FUEL TYPE METRICS ===
    print("\n2. PER FUEL TYPE METRICS:")
    fuel_metrics = {}

    for i, fuel in enumerate(fuel_columns):
        # Extract all predictions for this fuel type
        fuel_true = y_true[:, :, i].flatten()  # All samples, all time steps, specific fuel
        fuel_pred = y_pred[:, :, i].flatten()

        # Calculate metrics
        fuel_mse = mean_squared_error(fuel_true, fuel_pred)
        fuel_rmse = np.sqrt(fuel_mse)
        fuel_mae = mean_absolute_error(fuel_true, fuel_pred)

        # R² can be undefined if variance is 0 (fuel not used)
        if np.var(fuel_true) > 0:
            fuel_r2 = r2_score(fuel_true, fuel_pred)
        else:
            fuel_r2 = np.nan

        fuel_metrics[fuel] = {
            'mse': fuel_mse,
            'rmse': fuel_rmse,
            'mae': fuel_mae,
            'r2': fuel_r2
        }

        print(f"\n   {fuel}:")
        print(f"      MSE:  {fuel_mse:,.2f} MW²")
        print(f"      RMSE: {fuel_rmse:,.2f} MW")
        print(f"      MAE:  {fuel_mae:,.2f} MW")
        if not np.isnan(fuel_r2):
            print(f"      R²:   {fuel_r2:.4f}")
        else:
            print(f"      R²:   N/A (no variation in data)")

    # === METRICS BY TIME HORIZON ===
    print("\n3. ACCURACY BY FORECAST HORIZON:")
    horizons = [
        ("First 10 minutes", 0, 1),
        ("First 1 hour", 0, 6),
        ("First 6 hours", 0, 36),
        ("First 12 hours", 0, 72),
        ("Last 12 hours", 72, 144),
        ("Full 24 hours", 0, 144)
    ]

    horizon_metrics = {}
    for name, start_idx, end_idx in horizons:
        # Get predictions for this time range
        horizon_true = y_true[:, start_idx:end_idx, :].flatten()
        horizon_pred = y_pred[:, start_idx:end_idx, :].flatten()

        h_mse = mean_squared_error(horizon_true, horizon_pred)
        h_rmse = np.sqrt(h_mse)
        h_mae = mean_absolute_error(horizon_true, horizon_pred)

        horizon_metrics[name] = {
            'mse': h_mse,
            'rmse': h_rmse,
            'mae': h_mae
        }

        print(f"\n   {name}:")
        print(f"      MSE:  {h_mse:,.2f} MW²")
        print(f"      RMSE: {h_rmse:,.2f} MW")
        print(f"      MAE:  {h_mae:,.2f} MW")

    # === ERROR DEGRADATION ANALYSIS ===
    print("\n4. ERROR DEGRADATION OVER TIME:")
    # Calculate average error for each time step
    time_step_errors = []
    for t in range(144):
        t_true = y_true[:, t, :].flatten()
        t_pred = y_pred[:, t, :].flatten()
        t_mae = mean_absolute_error(t_true, t_pred)
        time_step_errors.append(t_mae)

    # Show degradation at key intervals
    print(f"   MAE at 10 min:  {time_step_errors[0]:.2f} MW")
    print(f"   MAE at 1 hour:  {time_step_errors[5]:.2f} MW")
    print(f"   MAE at 6 hours: {time_step_errors[35]:.2f} MW")
    print(f"   MAE at 12 hours: {time_step_errors[71]:.2f} MW")
    print(f"   MAE at 24 hours: {time_step_errors[143]:.2f} MW")

    # Calculate degradation rate
    degradation_rate = (time_step_errors[143] - time_step_errors[0]) / time_step_errors[0] * 100
    print(f"\n   Error increase from 10min to 24hr: {degradation_rate:.1f}%")

    return {
        'overall': {'mse': overall_mse, 'rmse': overall_rmse, 'mae': overall_mae, 'r2': overall_r2},
        'per_fuel': fuel_metrics,
        'per_horizon': horizon_metrics,
        'degradation': time_step_errors
    }

# =====================================
# MAIN EXECUTION FUNCTION
# =====================================

def main():
    """
    Main function to process all regions and train models
    """
    # Define regions and their file paths
    regions = {
        'North': 'North.csv',
        'Central': 'Central.csv',
        'South': 'South.csv',
        'East': 'East.csv',
        'Other': 'Other.csv'
    }

    # Store results for all regions
    all_results = {}

    # Process each region
    for region_name, filepath in regions.items():
        try:
            print(f"\n{'#'*80}")
            print(f"# PROCESSING {region_name.upper()} REGION")
            print('#'*80)

            # Step 1: Data Engineering
            df = process_single_csv(filepath, region_name)

            # Step 2: Train Model
            model_data = train_single_model(
                df,
                region_name,
                epochs=30,  # Reduced for faster training, increase to 50-100 for better results
                batch_size=32
            )

            # Step 3: Calculate Errors
            errors = calculate_errors(
                model_data['y_true'],
                model_data['y_pred'],
                model_data['fuel_columns'],
                region_name
            )

            # Store results
            all_results[region_name] = {
                'model_data': model_data,
                'errors': errors,
                'processed_df': df
            }

            # Save model
            model_filename = f'model_{region_name.lower()}.h5'
            model_data['model'].save(model_filename)
            print(f"\nModel saved as: {model_filename}")

        except FileNotFoundError:
            print(f"\nERROR: Could not find {filepath}")
            print(f"Skipping {region_name} region...")
            continue
        except Exception as e:
            print(f"\nERROR processing {region_name}: {str(e)}")
            print(f"Skipping {region_name} region...")
            continue

    # === FINAL SUMMARY ===
    print(f"\n{'#'*80}")
    print("# FINAL SUMMARY - ALL REGIONS")
    print('#'*80)

    print("\nOVERALL RMSE BY REGION:")
    for region, results in all_results.items():
        rmse = results['errors']['overall']['rmse']
        print(f"   {region}: {rmse:.2f} MW")

    print("\nMODELS TRAINED AND SAVED:")
    for region in all_results.keys():
        print(f"   - model_{region.lower()}.h5")

    return all_results

# =====================================
# RUN THE PIPELINE
# =====================================

if __name__ == "__main__":
    # Run the entire pipeline
    results = main()

    print("\n" + "="*80)
    print("TRAINING COMPLETE!")
    print("="*80)

